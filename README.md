# ETL-Data-Pipelines-with-Bash-Operator-using-Apache-Airflow
**Overview**

This project focuses on building an automated ETL pipeline using Apache Airflow to consolidate toll-plaza traffic data coming from multiple highway operators and multiple file formats into a single, standardized dataset.

The pipeline was designed to solve a very common real-world data engineering problem: operational data rarely arrives in one clean format. Instead, different systems generate data as CSVs, TSVs, or fixed-width text files, making manual consolidation slow, error-prone, and difficult to scale.

By orchestrating the entire workflow in Airflow, this project demonstrates how raw, heterogeneous data can be reliably transformed into a unified dataset that is ready for downstream analytics and traffic optimization efforts.

**Business Problem**

National highway toll data was generated by multiple toll plazas, each operated by different teams and systems. As a result:

Vehicle data was delivered as CSV files

Toll-plaza metadata arrived in TSV format

Payment details were stored in fixed-width text files

Because the data arrived in different formats and structures, analysts could not easily combine it to analyze traffic volume, vehicle types, or congestion patterns across highways.

Manual processing was time-consuming, inconsistent, and not repeatable. As traffic volumes increased, there was a clear need for an automated, scheduled ETL pipeline that could reliably prepare this data for analysis.

**Project Goal**

The goal of this project was to:

Automate ingestion of toll data across multiple file formats

Standardize and align fields across disparate data sources

Consolidate all extracted data into a single structured dataset

Orchestrate the entire workflow using Apache Airflow

Create a repeatable pipeline that can run daily without manual intervention

**Pipeline Overview**

The ETL pipeline is orchestrated as a single Airflow DAG (ETL_toll_data) and follows a clear, linear workflow:

Extract raw files from a compressed archive

Parse relevant fields from CSV, TSV, and fixed-width files

Combine all extracted fields into one consolidated dataset

Apply lightweight transformations to standardize the data

Produce a final output file ready for downstream analytics

Each step is implemented as an individual Airflow task, making the pipeline easy to monitor, debug, and extend.

**Data Extraction Strategy**

Different extraction techniques were used based on the input format:

CSV data: Extracted specific columns using delimiter-based parsing

TSV data: Parsed required fields using tab separators

Fixed-width data: Extracted precise character ranges to capture payment attributes

Using Unix utilities (cut, paste, tr) allowed the pipeline to remain lightweight, fast, and easy to reason aboutâ€”while still being production-relevant.

**Transformation Logic**

Once the raw fields were extracted and consolidated:

Vehicle type fields were standardized by converting text to uppercase

Column alignment was enforced to ensure consistency across sources

The final dataset was written as a clean, structured output file

These transformations ensure the dataset is analytics-ready and consistent across daily runs.

**Why Apache Airflow?**

Airflow was chosen to orchestrate this workflow because it provides:

Clear task dependency management

Retry and failure handling

Scheduling for recurring ETL runs

Visibility into pipeline execution through the UI

By modeling each ETL step as a separate task, the pipeline becomes easier to maintain and extend as data volume or complexity grows.

Validation & Operational Checks

**After deployment, the pipeline was validated by:**

Confirming DAG registration within Airflow

Verifying all tasks appeared correctly in the DAG view

Executing the DAG and monitoring task-level success

Inspecting the final output file for correctness and consistency

These steps ensured the pipeline was not only functional, but operationally reliable.

**Business Impact**

This pipeline enables:

Reliable consolidation of toll data across multiple operators

Elimination of manual, error-prone data preparation steps

Faster access to clean traffic data for analysis

A scalable foundation for future analytics on congestion patterns and highway usage

By automating ingestion and standardization, analysts can focus on insights instead of data cleanup.

**Key Engineering Takeaways**

Designed a real-world ETL pipeline handling heterogeneous data sources

Implemented orchestration using Apache Airflow and task dependencies

Applied Unix-based transformations for efficient data processing

Built a repeatable and maintainable data pipeline suitable for production environments

